{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUE-CLUENER 细粒度命名实体识别\n",
    "\n",
    "本数据是在清华大学开源的文本分类数据集THUCTC基础上，选出部分数据进行细粒度命名实体标注，原数据来源于Sina News RSS.\n",
    "\n",
    "训练集：10748 验证集：1343\n",
    "\n",
    "标签类别：\n",
    "数据分为10个标签类别，分别为: 地址（address），书名（book），公司（company），游戏（game），政府（goverment），电影（movie），姓名（name），组织机构（organization），职位（position），景点（scene）\n",
    "\n",
    "数据下载地址：https://github.com/CLUEbenchmark/CLUENER2020\n",
    "\n",
    "排行榜地址：https://cluebenchmarks.com/ner.html\n",
    "\n",
    "|模型|线上效果f1|\n",
    "|------|------:|\n",
    "|Bert-base|78.82|\n",
    "|RoBERTa-wwm-large-ext|80.42|\n",
    "|Bi-Lstm + CRF|70.00|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据观察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idleuncle/.pyenv/versions/env-nlp/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "seg_len=0\n",
    "seg_backoff=0\n",
    "fold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './data/rawdata/train.json'\n",
    "test_file = './data/rawdata/test.json'\n",
    "dev_file = './data/rawdata/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(json_file):\n",
    "    rd = open(json_file, 'r')\n",
    "    lines = rd.readlines()\n",
    "    rd.close()\n",
    "    json_data = []\n",
    "    for line in tqdm(lines):\n",
    "        line = line.strip()\n",
    "        line_data = json.loads(line)\n",
    "        json_data.append(line_data)\n",
    "    print(f\"Total: {len(json_data)}\")\n",
    "    print(json_data[:5])\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10748/10748 [00:00<00:00, 169188.25it/s]\n",
      "100%|██████████| 1345/1345 [00:00<00:00, 319912.61it/s]\n",
      "100%|██████████| 1343/1343 [00:00<00:00, 33401.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 10748\n",
      "[{'text': '浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，', 'label': {'name': {'叶老桂': [[9, 11]]}, 'company': {'浙商银行': [[0, 3]]}}}, {'text': '生生不息CSOL生化狂潮让你填弹狂扫', 'label': {'game': {'CSOL': [[4, 7]]}}}, {'text': '那不勒斯vs锡耶纳以及桑普vs热那亚之上呢？', 'label': {'organization': {'那不勒斯': [[0, 3]], '锡耶纳': [[6, 8]], '桑普': [[11, 12]], '热那亚': [[15, 17]]}}}, {'text': '加勒比海盗3：世界尽头》的去年同期成绩死死甩在身后，后者则即将赶超《变形金刚》，', 'label': {'movie': {'加勒比海盗3：世界尽头》': [[0, 11]], '《变形金刚》': [[33, 38]]}}}, {'text': '布鲁京斯研究所桑顿中国中心研究部主任李成说，东亚的和平与安全，是美国的“核心利益”之一。', 'label': {'address': {'美国': [[32, 33]]}, 'organization': {'布鲁京斯研究所桑顿中国中心': [[0, 12]]}, 'name': {'李成': [[18, 19]]}, 'position': {'研究部主任': [[13, 17]]}}}]\n",
      "Total: 1345\n",
      "[{'id': 0, 'text': '四川敦煌学”。近年来，丹棱县等地一些不知名的石窟迎来了海内外的游客，他们随身携带着胡文和的著作。'}, {'id': 1, 'text': '尼日利亚海军发言人当天在阿布贾向尼日利亚通讯社证实了这一消息。'}, {'id': 2, 'text': '销售冠军：辐射3-Bethesda'}, {'id': 3, 'text': '所以大多数人都是从巴厘岛南部开始环岛之旅。'}, {'id': 4, 'text': '备受瞩目的动作及冒险类大作《迷失》在其英文版上市之初就受到了全球玩家的大力追捧。'}]\n",
      "Total: 1343\n",
      "[{'text': '彭小军认为，国内银行现在走的是台湾的发卡模式，先通过跑马圈地再在圈的地里面选择客户，', 'label': {'address': {'台湾': [[15, 16]]}, 'name': {'彭小军': [[0, 2]]}}}, {'text': '温格的球队终于又踢了一场经典的比赛，2比1战胜曼联之后枪手仍然留在了夺冠集团之内，', 'label': {'organization': {'曼联': [[23, 24]]}, 'name': {'温格': [[0, 1]]}}}, {'text': '突袭黑暗雅典娜》中Riddick发现之前抓住他的赏金猎人Johns，', 'label': {'game': {'突袭黑暗雅典娜》': [[0, 7]]}, 'name': {'Riddick': [[9, 15]], 'Johns': [[28, 32]]}}}, {'text': '郑阿姨就赶到文汇路排队拿钱，希望能将缴纳的一万余元学费拿回来，顺便找校方或者教委要个说法。', 'label': {'address': {'文汇路': [[6, 8]]}}}, {'text': '我想站在雪山脚下你会被那巍峨的雪山所震撼，但你一定要在自己身体条件允许的情况下坚持走到牛奶海、', 'label': {'scene': {'牛奶海': [[43, 45]], '雪山': [[4, 5]]}}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = load_json_data(train_file)\n",
    "test_data = load_json_data(test_file)\n",
    "dev_data = load_json_data(dev_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 样本数量分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train_data + dev_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 样本长度分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12091/12091 [00:00<00:00, 2158932.72it/s]\n",
      "2020-06-01 13:48:13.393 | INFO     | __main__:<module>:2 - ***** Text Lengths *****\n",
      "2020-06-01 13:48:13.395 | INFO     | __main__:<module>:3 - mean: 37.39\n",
      "2020-06-01 13:48:13.396 | INFO     | __main__:<module>:4 - std: 37.39\n",
      "2020-06-01 13:48:13.398 | INFO     | __main__:<module>:5 - max: 50\n",
      "2020-06-01 13:48:13.399 | INFO     | __main__:<module>:6 - min: 2\n"
     ]
    }
   ],
   "source": [
    "lengths = [ len(x['text']) for x in tqdm(all_data)]\n",
    "logger.info(f\"***** Text Lengths *****\")\n",
    "logger.info(f\"mean: {np.mean(lengths):.2f}\")\n",
    "logger.info(f\"std: {np.mean(lengths):.2f}\")\n",
    "logger.info(f\"max: {np.max(lengths)}\")\n",
    "logger.info(f\"min: {np.min(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 样本标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12091/12091 [00:00<00:00, 1282745.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'name': 3199, 'position': 2811, 'company': 2494, 'address': 2363, 'game': 2123, 'organization': 2100, 'government': 1651, 'scene': 1070, 'book': 1029, 'movie': 880})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "for text_data in tqdm(all_data):\n",
    "    labels = text_data['label']\n",
    "    for k, v in labels.items():\n",
    "        all_labels.append(k)\n",
    "print(f\"{Counter(all_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['address',\n",
       " 'book',\n",
       " 'company',\n",
       " 'game',\n",
       " 'government',\n",
       " 'movie',\n",
       " 'name',\n",
       " 'organization',\n",
       " 'position',\n",
       " 'scene']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_labels = np.unique(all_labels).tolist()\n",
    "ner_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, random\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "\n",
    "from theta.utils import init_theta, split_train_eval_examples\n",
    "from theta.modeling.ner_span import NerTrainer, load_model, load_examples, init_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 模型输入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def labeling_text_bios(text, entities):\n",
    "    \"\"\"\n",
    "    text: str\n",
    "\n",
    "        \"万通地产设计总监刘克峰；\"\n",
    "\n",
    "    entities: [(role, mention, s, e), ...]\n",
    "            \n",
    "        [(\"name\", 刘克峰\", 8, 10), (\"company\", \"万通地产\", 0, 3), ...]\n",
    "\n",
    "    output:\n",
    "\n",
    "         \n",
    "    \"\"\"\n",
    "    words = [w for w in text]\n",
    "    labels = ['O'] * len(words)\n",
    "\n",
    "    for entity in entities:\n",
    "        role, mention, s, e = entity\n",
    "        assert s <= e\n",
    "        mention_len = e - s + 1\n",
    "        if mention_len == 1:\n",
    "            labels[s] = f\"S-{role}\"\n",
    "        else:\n",
    "            labels[s] = f\"B-{role}\"\n",
    "            for j0 in range(1, mention_len):\n",
    "                labels[s + j0] = f\"I-{role}\"\n",
    "    return labels\n",
    "\n",
    "def labeling_text_span(text, entities):\n",
    "    \"\"\"\n",
    "    text: str\n",
    "\n",
    "        \"万通地产设计总监刘克峰；\"\n",
    "\n",
    "    entities: [(role, mention, s, e), ...]\n",
    "            \n",
    "        [(\"name\", 刘克峰\", 8, 10), (\"company\", \"万通地产\", 0, 3), ...]\n",
    "\n",
    "    output:\n",
    "\n",
    "         \n",
    "    \"\"\"\n",
    "    labels = []\n",
    "\n",
    "    for entity in entities:\n",
    "        role, mention, s, e = entity\n",
    "        assert s <= e\n",
    "        labels.append((role, s, e))\n",
    "        \n",
    "    return labels\n",
    "\n",
    "def train_data_generator(args, train_file, seg_len=0, seg_backoff=0):\n",
    "    \"\"\"\n",
    "    每行一条json格式数据。\n",
    "    \"\"\"\n",
    "\n",
    "#     guid = 0\n",
    "#     examples = []\n",
    "#     with open(args.train_file, 'r') as fr:\n",
    "#         lines = fr.readlines()\n",
    "#         for i, line in enumerate(tqdm(lines, desc=f\"train & eval\")):\n",
    "#             d = json.loads(line)\n",
    "\n",
    "#             # -------------------- 自定义json格式 --------------------\n",
    "#             #  {\n",
    "#             #      \"text\": \"万通地产设计总监刘克峰；\",\n",
    "#             #      \"label\": {\n",
    "#             #          \"name\": {\n",
    "#             #              \"刘克峰\": [[8, 10]]\n",
    "#             #          },\n",
    "#             #          \"company\": {\n",
    "#             #              \"万通地产\": [[0, 3]]\n",
    "#             #          },\n",
    "#             #          \"position\": {\n",
    "#             #              \"设计总监\": [[4, 7]]\n",
    "#             #          }\n",
    "#             #      }\n",
    "#             #  }\n",
    "        \n",
    "    all_data = train_data+dev_data\n",
    "    all_labels = []\n",
    "    \n",
    "    total_examples = len(all_data)\n",
    "    num_sample_examples = int(total_examples * args.train_sample_rate)\n",
    "    logger.warning(\n",
    "        f\"Sample {num_sample_examples}/{total_examples} ({args.train_sample_rate*100:.1f}%) train examples.\"\n",
    "    )\n",
    "\n",
    "    for i, d in enumerate(tqdm(all_data, desc=\"train\")):\n",
    "        if i >= num_sample_examples:\n",
    "            break        \n",
    "\n",
    "        text = d['text']\n",
    "        text = clean_text(text)\n",
    "\n",
    "        entities = []\n",
    "        classes = d['label'].keys()\n",
    "        for c in classes:\n",
    "            c_labels = d['label'][c]\n",
    "            #  logger.debug(f\"c_labels:{c_labels}\")\n",
    "            for label, span in c_labels.items():\n",
    "                x0, x1 = span[0]\n",
    "                entities.append((c, x0, x1))\n",
    "                guid = f\"{i}\"\n",
    "\n",
    "                #examples.append({\n",
    "                #    'guid': guid,\n",
    "                #    'text': text,\n",
    "                #    'entities': entities\n",
    "                #})\n",
    "                yield guid, text, None, entities\n",
    "                    \n",
    "    \n",
    "def load_train_val_examples(args, seg_len=0, seg_backoff=0):\n",
    "    train_base_examples = load_examples(args,\n",
    "                                        train_data_generator,\n",
    "                                        args.train_file,\n",
    "                                        seg_len=seg_len,\n",
    "                                        seg_backoff=seg_backoff)\n",
    "#     logger.debug(f\"{train_base_examples[:10]}\")\n",
    "\n",
    "    train_examples, val_examples = split_train_eval_examples(\n",
    "        train_base_examples,\n",
    "        train_rate=args.train_rate,\n",
    "        fold=args.fold,\n",
    "        shuffle=True,\n",
    "        random_state=args.seed)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Loaded {len(train_examples)} train examples, {len(val_examples)} val examples.\"\n",
    "    )\n",
    "    return train_examples, val_examples\n",
    "    \n",
    "\n",
    "def load_test_examples(args, seg_len=0, seg_backoff=0):\n",
    "    from theta.modeling.ner import InputExample\n",
    "\n",
    "    test_examples = []\n",
    "    with open(args.test_file, 'r') as fr:\n",
    "        lines = fr.readlines()\n",
    "        for i, line in enumerate(tqdm(lines, desc=f\"train & eval\")):\n",
    "            d = json.loads(line)\n",
    "\n",
    "            # -------------------- 自定义json格式 --------------------\n",
    "            #  {\n",
    "            #      \"id\": 1,\n",
    "            #      \"text\": \"尼日利亚海军发言人当天在阿布贾向尼日利亚通讯社证实了这一消息。\"\n",
    "            #  }\n",
    "\n",
    "            guid = str(d['id'])\n",
    "            text = d['text']\n",
    "            text = clean_text(text)\n",
    "\n",
    "            test_examples.append(\n",
    "                InputExample(guid=guid, text_a=text, labels=None))\n",
    "\n",
    "    logger.info(f\"Loaded {len(test_examples)} test examples.\")\n",
    "    return test_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 模型输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predict_results(args, pred_results, test_examples):\n",
    "    from theta.utils import get_pred_results_file\n",
    "    pred_results_file = get_pred_results_file(args)\n",
    "\n",
    "    test_results = {}\n",
    "    for json_d, example in tqdm(zip(pred_results, test_examples)):\n",
    "        guid = example.guid\n",
    "        text = ''.join(example.text_a)\n",
    "\n",
    "        if guid not in test_results:\n",
    "            test_results[guid] = {\n",
    "                \"guid\": guid,\n",
    "                \"content\": \"\",\n",
    "                \"events\": [],\n",
    "                \"tagged_text\": \"\"\n",
    "            }\n",
    "\n",
    "        s0 = 0\n",
    "        tagged_text = test_results[guid]['tagged_text']\n",
    "        text_offset = len(test_results[guid]['content'])\n",
    "        for entity in json_d['entities']:\n",
    "            event_type = entity[0]\n",
    "            s = entity[1]\n",
    "            e = entity[2] + 1\n",
    "            entity_text = text[s:e]\n",
    "            test_results[guid]['events'].append(\n",
    "                (event_type, entity_text, text_offset + s, text_offset + e))\n",
    "\n",
    "            tagged_text += f\"{text[s0:s]}\\n\"\n",
    "            tagged_text += f\"【{event_type} | {entity_text}】\\n\"\n",
    "            s0 = e\n",
    "\n",
    "        tagged_text += f\"{text[s0:]}\\n\"\n",
    "        test_results[guid]['tagged_text'] = tagged_text\n",
    "        test_results[guid]['content'] += text\n",
    "\n",
    "    json.dump(test_results,\n",
    "              open(f\"{pred_results_file}\", 'w'),\n",
    "              ensure_ascii=False,\n",
    "              indent=2)\n",
    "    logger.info(f\"Saved predict results to {pred_results_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 自定义模型\n",
    "Theta对每类任务都有缺省模型，通常情况下不需要自定义模型。训练器Trainer中传入参数build_model=None即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Model --------------------\n",
    "\n",
    "\n",
    "def build_model(args):\n",
    "    \"\"\"\n",
    "    自定义模型\n",
    "    规格要求返回模型(model)、优化器(optimizer)、调度器(scheduler)三元组。\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------- model --------\n",
    "    from theta.modeling.ner_span import load_pretrained_model\n",
    "    model = load_pretrained_model(args)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # -------- optimizer --------\n",
    "    from transformers.optimization import AdamW\n",
    "    from theta.modeling.trainer import get_default_optimizer_parameters\n",
    "    optimizer_parameters = get_default_optimizer_parameters(\n",
    "        model, args.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters,\n",
    "                      lr=args.learning_rate,\n",
    "                      correct_bias=False)\n",
    "\n",
    "    # -------- scheduler --------\n",
    "    from transformers.optimization import get_linear_schedule_with_warmup\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=args.total_steps * args.warmup_rate,\n",
    "        num_training_steps=args.total_steps)\n",
    "\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 自定训练器\n",
    "\n",
    "训练器也不是必须定义的，可以直接用NerTrainer实例化训练器。\n",
    "\n",
    "自定义训练器通常是为了使用自定义模型或重载训练、评估、推理过程的关键节点，便于输出、调试等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Trainer --------------------\n",
    "\n",
    "from theta.modeling.ner_span import NerTrainer\n",
    "\n",
    "class AppTrainer(NerTrainer):\n",
    "    def __init__(self, args):\n",
    "        # 使用自定义模型时，传入build_model参数。\n",
    "        super(AppTrainer, self).__init__(args, build_model=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 主控流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    init_theta(args)\n",
    "    init_labels(args, ner_labels)\n",
    "\n",
    "    trainer = AppTrainer(args)\n",
    "\n",
    "    # --------------- train phase ---------------\n",
    "    if args.do_train:\n",
    "        train_examples, val_examples = load_train_val_examples(\n",
    "            args, seg_len=seg_len, seg_backoff=seg_backoff)\n",
    "        trainer.train(args, train_examples, val_examples)\n",
    "    # --------------- predict phase ---------------\n",
    "    if args.do_predict:\n",
    "        test_examples = load_test_examples(args,\n",
    "                                           seg_len=seg_len,\n",
    "                                           seg_backoff=seg_backoff)\n",
    "\n",
    "        model = load_model(args)\n",
    "        trainer.predict(args, model, test_examples)\n",
    "        save_predict_results(args, trainer.pred_results, f\"{args.dataset_name}_predict.json\",\n",
    "                             test_examples)\n",
    "    # --------------- evaluate phase ---------------\n",
    "    if args.do_eval:\n",
    "#        eval_examples = load_eval_examples(args,\n",
    "#                                           seg_len=seg_len,\n",
    "#                                           seg_backoff=seg_backoff)\n",
    "        train_examples, eval_examples = load_train_val_examples(\n",
    "            args, seg_len=seg_len, seg_backoff=seg_backoff)\n",
    "\n",
    "        model = load_model(args)\n",
    "        trainer.evaluate(args, model, eval_examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 全局参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def add_special_args(parser):\n",
    "#    return parser\n",
    "\n",
    "#from theta.modeling.glue.args import get_args\n",
    "#args = get_args([add_special_args])\n",
    "\n",
    "import sys, argparse\n",
    "\n",
    "def get_init_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    for arg in sys.argv:\n",
    "        if arg.startswith('-'):\n",
    "            parser.add_argument(arg, type=str)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "#import argparse\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"-f\",type=str)\n",
    "#args = parser.parse_args()\n",
    "\n",
    "args = get_init_args()\n",
    "\n",
    "DATASET_NAME=\"cluener\"\n",
    "DATA_DIR=\"./data\"\n",
    "OUTPUT_DIR=f\"output_{DATASET_NAME}\"\n",
    "CHECKPOINT_MODEL=f\"{OUTPUT_DIR}/best\"\n",
    "\n",
    "TRAIN_FILE = \"./data/rawdata/train.json\"\n",
    "TEST_FILE = \"./data/rawdatda/test.json\"\n",
    "EVAL_FILE = \"./data/rawdata/eval.json\"\n",
    "\n",
    "EPOCHS=10\n",
    "TRAIN_SAMPLE_RATE=1.0\n",
    "\n",
    "MODEL_TYPE=\"bert\"\n",
    "PRETRAINED_MODEL=\"/opt/share/pretrained/pytorch/bert-base-chinese\"\n",
    "LEARNING_RATE=2e-5\n",
    "TRAIN_MAX_SEQ_LENGTH=64\n",
    "EVAL_MAX_SEQ_LENGTH=64\n",
    "TRAIN_BATCH_SIZE=128\n",
    "EVAL_BATCH_SIZE=64\n",
    "PREDICT_BATCH_SIZE=64\n",
    "\n",
    "args.do_train=False\n",
    "args.do_predict=False\n",
    "args.do_eval=False\n",
    "args.train_max_seq_length = TRAIN_MAX_SEQ_LENGTH\n",
    "args.eval_max_seq_length = EVAL_MAX_SEQ_LENGTH\n",
    "args.num_train_epochs = EPOCHS\n",
    "args.learning_rate = LEARNING_RATE\n",
    "args.per_gpu_train_batch_size = TRAIN_BATCH_SIZE\n",
    "args.per_gpu_eval_batch_size = EVAL_BATCH_SIZE\n",
    "args.per_gpu_predict_batch_size = EVAL_BATCH_SIZE\n",
    "\n",
    "args.data_dir = DATA_DIR\n",
    "args.dataset_name = DATASET_NAME\n",
    "args.train_file = TRAIN_FILE\n",
    "args.eval_file = EVAL_FILE\n",
    "args.test_file = TEST_FILE\n",
    "\n",
    "args.output_dir = OUTPUT_DIR\n",
    "args.pred_output_dir = OUTPUT_DIR\n",
    "\n",
    "args.model_type = MODEL_TYPE\n",
    "args.model_path = PRETRAINED_MODEL\n",
    "args.overwrite_cache = True\n",
    "args.train_sample_rate = TRAIN_SAMPLE_RATE\n",
    "args.seed = 8864\n",
    "args.local_rank=-1\n",
    "args.no_cuda = None\n",
    "args.do_lower_case=True\n",
    "args.cache_dir = None\n",
    "args.train_rate=0.8\n",
    "args.fold = 0\n",
    "args.gradient_accumulation_steps = 1\n",
    "args.max_steps = 0\n",
    "args.focalloss_gamma = 1.5\n",
    "args.focalloss_alpha = None\n",
    "args.weight_decay = 0.0\n",
    "args.warmup_rate = 0.1\n",
    "args.fp16 = True\n",
    "args.fp16_opt_level = 'O1'\n",
    "args.max_grad_norm = 1.0\n",
    "args.save_checkpoints = False\n",
    "args.no_eval_on_each_epoch=False\n",
    "\n",
    "args.soft_label = True\n",
    "args.loss_type = 'CrossEntropyLoss'\n",
    "#args.loss_type = 'FocalLoss'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-01 13:48:19.481 | INFO     | theta.modeling.ner_span.dataset:init_labels:100 - args.label2id: {'[unused1]': 0, 'address': 1, 'book': 2, 'company': 3, 'game': 4, 'government': 5, 'movie': 6, 'name': 7, 'organization': 8, 'position': 9, 'scene': 10}\n",
      "2020-06-01 13:48:19.482 | INFO     | theta.modeling.ner_span.dataset:init_labels:101 - args.id2label: {0: '[unused1]', 1: 'address', 2: 'book', 3: 'company', 4: 'game', 5: 'government', 6: 'movie', 7: 'name', 8: 'organization', 9: 'position', 10: 'scene'}\n",
      "2020-06-01 13:48:19.483 | INFO     | theta.modeling.ner_span.dataset:init_labels:102 - args.num_labels: 11\n",
      "2020-06-01 13:48:19.520 | WARNING  | __main__:train_data_generator:92 - Sample 12091/12091 (100.0%) train examples.\n",
      "train: 100%|██████████| 12091/12091 [00:00<00:00, 71062.90it/s]\n",
      "2020-06-01 13:48:19.695 | INFO     | theta.modeling.ner_span.dataset:load_examples:88 - Loaded 26320 examples.\n",
      "2020-06-01 13:48:19.701 | INFO     | __main__:load_train_val_examples:136 - Loaded 21057 train examples, 5263 val examples.\n",
      "2020-06-01 13:48:19.703 | INFO     | theta.modeling.trainer:train:136 - Start train: 21057 train examples, 5263 eval examples.\n",
      "Tokenize: 100%|██████████| 21057/21057 [00:00<00:00, 71247.78it/s]\n",
      "2020-06-01 13:48:21.205 | INFO     | theta.modeling.trainer:train:150 - Start training ...\n",
      "2020-06-01 13:48:21.206 | INFO     | theta.modeling.trainer:train:151 -   Num examples    = 21057\n",
      "2020-06-01 13:48:21.207 | INFO     | theta.modeling.trainer:train:152 -   Num epoch steps = 165\n",
      "2020-06-01 13:48:21.207 | INFO     | theta.modeling.trainer:train:153 -   Num epochs = 10\n",
      "2020-06-01 13:48:21.208 | INFO     | theta.modeling.trainer:train:154 -   Batch size = 128\n",
      "2020-06-01 13:48:21.209 | INFO     | theta.modeling.trainer:train:167 -   Gradient Accumulation steps = 1\n",
      "2020-06-01 13:48:21.210 | INFO     | theta.modeling.trainer:train:169 -   Total optimization steps = 1650\n",
      "2020-06-01 13:48:21.212 | INFO     | theta.modeling.ner_span.trainer:load_pretrained_model:189 - model_path: /opt/share/pretrained/pytorch/bert-base-chinese\n",
      "2020-06-01 13:48:21.213 | INFO     | theta.modeling.ner_span.trainer:load_pretrained_model:190 - config:BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"focalloss_alpha\": null,\n",
      "  \"focalloss_gamma\": 1.5,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"loss_type\": \"CrossEntropyLoss\",\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"soft_label\": true,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idleuncle/.pyenv/versions/env-nlp/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch(1/10)   1/165 [..............................] - ETA: 8:59 - lr: 0.00e+00 - loss: 2.5604Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idleuncle/.pyenv/versions/env-nlp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch(1/10) 165/165 [==============================] - 62s 377ms/step - lr: 9.94e-06 - loss: 0.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-01 13:49:28.829 | INFO     | theta.modeling.trainer:train:314 - Epoch(1/10) evaluating.\n",
      "Tokenize: 100%|██████████| 5263/5263 [00:00<00:00, 65643.97it/s]\n",
      "2020-06-01 13:49:29.147 | INFO     | theta.modeling.trainer:evaluate:397 - Start evaluating ...\n",
      "2020-06-01 13:49:29.148 | INFO     | theta.modeling.trainer:evaluate:398 -   Num examples    = 5263\n",
      "2020-06-01 13:49:29.149 | INFO     | theta.modeling.trainer:evaluate:399 -   Num epoch steps = 83\n",
      "2020-06-01 13:49:29.150 | INFO     | theta.modeling.trainer:evaluate:400 -   Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 83/83 [==============================] - 141s 2s/step - acc: 0.8330 - recall: 0.8525 - f1: 0.8426 - loss: 0.0421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-01 13:51:50.646 | INFO     | theta.utils.ner_utils:get_ner_results:13 - =======================================================\n",
      "2020-06-01 13:51:50.647 | INFO     | theta.utils.ner_utils:get_ner_results:14 -                                    acc    recall f1    \n",
      "2020-06-01 13:51:50.648 | INFO     | theta.utils.ner_utils:get_ner_results:15 - -------------------------------------------------------\n",
      "2020-06-01 13:51:50.649 | INFO     | theta.utils.ner_utils:get_ner_results:26 - name                             | 0.8916 0.9273 0.9091\n",
      "2020-06-01 13:51:50.649 | INFO     | theta.utils.ner_utils:get_ner_results:26 - book                             | 0.8323 0.9177 0.8729\n",
      "2020-06-01 13:51:50.650 | INFO     | theta.utils.ner_utils:get_ner_results:26 - company                          | 0.8681 0.8430 0.8553\n",
      "2020-06-01 13:51:50.650 | INFO     | theta.utils.ner_utils:get_ner_results:26 - game                             | 0.8454 0.8603 0.8528\n",
      "2020-06-01 13:51:50.651 | INFO     | theta.utils.ner_utils:get_ner_results:26 - position                         | 0.8269 0.8787 0.8521\n",
      "2020-06-01 13:51:50.652 | INFO     | theta.utils.ner_utils:get_ner_results:26 - government                       | 0.8277 0.8653 0.8461\n",
      "2020-06-01 13:51:50.652 | INFO     | theta.utils.ner_utils:get_ner_results:26 - organization                     | 0.8100 0.8555 0.8321\n",
      "2020-06-01 13:51:50.653 | INFO     | theta.utils.ner_utils:get_ner_results:26 - movie                            | 0.9283 0.7447 0.8265\n",
      "2020-06-01 13:51:50.653 | INFO     | theta.utils.ner_utils:get_ner_results:26 - scene                            | 0.7714 0.8426 0.8054\n",
      "2020-06-01 13:51:50.654 | INFO     | theta.utils.ner_utils:get_ner_results:26 - address                          | 0.7440 0.7258 0.7348\n",
      "2020-06-01 13:51:50.655 | INFO     | theta.utils.ner_utils:get_ner_results:27 - -------------------------------------------------------\n",
      "2020-06-01 13:51:50.655 | INFO     | theta.utils.ner_utils:get_ner_results:29 -                                  | 0.8330 0.8525 0.8426\n",
      "2020-06-01 13:51:50.656 | INFO     | theta.utils.ner_utils:get_ner_results:30 - -------------------------------------------------------\n",
      "2020-06-01 13:51:50.663 | INFO     | theta.modeling.trainer:train:352 - Best f1: 0.8426 (0.842641)\n",
      "2020-06-01 13:51:50.664 | INFO     | theta.modeling.trainer:save_model:111 - Saving model checkpoint to output_cluener/best\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_acc\": \"0.833036\", \"eval_recall\": \"0.852470\", \"eval_f1\": \"0.842641\", \"learning_rate\": \"0.000020\", \"loss\": \"0.355189\", \"step\": 165}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idleuncle/.pyenv/versions/env-nlp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Epoch(2/10) 165/165 [==============================] - 59s 359ms/step - lr: 1.89e-05 - loss: 0.0291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-01 13:52:56.754 | INFO     | theta.modeling.trainer:train:314 - Epoch(2/10) evaluating.\n",
      "Tokenize: 100%|██████████| 5263/5263 [00:00<00:00, 61145.32it/s]\n",
      "2020-06-01 13:52:57.082 | INFO     | theta.modeling.trainer:evaluate:397 - Start evaluating ...\n",
      "2020-06-01 13:52:57.083 | INFO     | theta.modeling.trainer:evaluate:398 -   Num examples    = 5263\n",
      "2020-06-01 13:52:57.083 | INFO     | theta.modeling.trainer:evaluate:399 -   Num epoch steps = 83\n",
      "2020-06-01 13:52:57.084 | INFO     | theta.modeling.trainer:evaluate:400 -   Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 83/83 [==============================] - 142s 2s/step - acc: 0.8876 - recall: 0.9202 - f1: 0.9036 - loss: 0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-01 13:55:19.300 | INFO     | theta.utils.ner_utils:get_ner_results:13 - =======================================================\n",
      "2020-06-01 13:55:19.301 | INFO     | theta.utils.ner_utils:get_ner_results:14 -                                    acc    recall f1    \n",
      "2020-06-01 13:55:19.302 | INFO     | theta.utils.ner_utils:get_ner_results:15 - -------------------------------------------------------\n",
      "2020-06-01 13:55:19.303 | INFO     | theta.utils.ner_utils:get_ner_results:26 - name                             | 0.9255 0.9618 0.9433\n",
      "2020-06-01 13:55:19.303 | INFO     | theta.utils.ner_utils:get_ner_results:26 - book                             | 0.9219 0.9393 0.9305\n",
      "2020-06-01 13:55:19.304 | INFO     | theta.utils.ner_utils:get_ner_results:26 - company                          | 0.8922 0.9209 0.9064\n",
      "2020-06-01 13:55:19.304 | INFO     | theta.utils.ner_utils:get_ner_results:26 - position                         | 0.8748 0.9383 0.9054\n",
      "2020-06-01 13:55:19.305 | INFO     | theta.utils.ner_utils:get_ner_results:26 - government                       | 0.8847 0.9217 0.9029\n",
      "2020-06-01 13:55:19.306 | INFO     | theta.utils.ner_utils:get_ner_results:26 - movie                            | 0.9266 0.8682 0.8965\n",
      "2020-06-01 13:55:19.306 | INFO     | theta.utils.ner_utils:get_ner_results:26 - scene                            | 0.8713 0.9208 0.8954\n",
      "2020-06-01 13:55:19.307 | INFO     | theta.utils.ner_utils:get_ner_results:26 - game                             | 0.8662 0.9213 0.8929\n",
      "2020-06-01 13:55:19.308 | INFO     | theta.utils.ner_utils:get_ner_results:26 - organization                     | 0.8415 0.9146 0.8765\n",
      "2020-06-01 13:55:19.308 | INFO     | theta.utils.ner_utils:get_ner_results:26 - address                          | 0.8915 0.8536 0.8721\n",
      "2020-06-01 13:55:19.309 | INFO     | theta.utils.ner_utils:get_ner_results:27 - -------------------------------------------------------\n",
      "2020-06-01 13:55:19.309 | INFO     | theta.utils.ner_utils:get_ner_results:29 -                                  | 0.8876 0.9202 0.9036\n",
      "2020-06-01 13:55:19.310 | INFO     | theta.utils.ner_utils:get_ner_results:30 - -------------------------------------------------------\n",
      "2020-06-01 13:55:19.317 | INFO     | theta.modeling.trainer:train:352 - Best f1: 0.9036 (0.060994)\n",
      "2020-06-01 13:55:19.318 | INFO     | theta.modeling.trainer:save_model:111 - Saving model checkpoint to output_cluener/best\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_acc\": \"0.887618\", \"eval_recall\": \"0.920240\", \"eval_f1\": \"0.903635\", \"learning_rate\": \"0.000018\", \"loss\": \"0.035475\", \"step\": 330}\n",
      " \n",
      "Epoch(3/10) 165/165 [==============================] - 59s 357ms/step - lr: 1.67e-05 - loss: 0.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-01 13:56:24.987 | INFO     | theta.modeling.trainer:train:314 - Epoch(3/10) evaluating.\n",
      "Tokenize: 100%|██████████| 5263/5263 [00:00<00:00, 65440.03it/s]\n",
      "2020-06-01 13:56:25.309 | INFO     | theta.modeling.trainer:evaluate:397 - Start evaluating ...\n",
      "2020-06-01 13:56:25.310 | INFO     | theta.modeling.trainer:evaluate:398 -   Num examples    = 5263\n",
      "2020-06-01 13:56:25.311 | INFO     | theta.modeling.trainer:evaluate:399 -   Num epoch steps = 83\n",
      "2020-06-01 13:56:25.312 | INFO     | theta.modeling.trainer:evaluate:400 -   Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 43/83 [==============>...............] - ETA: 1:09 - acc: 0.9133 - recall: 0.9483 - f1: 0.9305 - loss: 0.0110"
     ]
    }
   ],
   "source": [
    "args.do_train=True\n",
    "args.do_predict=False\n",
    "args.do_eval=False\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 启动推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.do_train=False\n",
    "args.do_predict=True\n",
    "args.do_eval=False\n",
    "args.model_path=CHECKPOINT_MODEL\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-nlp",
   "language": "python",
   "name": "env-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
